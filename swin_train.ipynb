{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df559d6ecf587ef4",
   "metadata": {},
   "source": [
    "# Swin for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6487069a22b443e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, mkdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebb5e60434fc75",
   "metadata": {},
   "source": [
    "### Huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96406df8351e29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment out the line below when you need to login to Huggingface\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88444867b0b14178",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e70cad906b1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5ca44145e20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f33be18440cabc",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41127ff906823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import (\n",
    "    ImageNet1K, CIFAR100, CIFAR10, DatasetHolder,\n",
    "    IMAGENET1KConfig, CIFAR100Config, CIFAR10Config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d472ec0525e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import (\n",
    "    ImageNet1K, CIFAR100, CIFAR10, DatasetHolder,\n",
    "    IMAGENET1KConfig, CIFAR100Config, CIFAR10Config\n",
    ")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# IMAGENETs = DatasetHolder(\n",
    "#     config=IMAGENET1KConfig,\n",
    "#     train=ImageNet1K(\n",
    "#         root=DATA_ROOT, force_download=False, train=True, transform=IMAGENET1KConfig.augmentation\n",
    "#     ),\n",
    "#     valid=ImageNet1K(\n",
    "#         root=DATA_ROOT, force_download=False, valid=True, transform=IMAGENET1KConfig.resizer\n",
    "#     ),\n",
    "#     test=ImageNet1K(\n",
    "#         root=DATA_ROOT, force_download=False, train=False, transform=IMAGENET1KConfig.resizer\n",
    "#     )\n",
    "# )\n",
    "# IMAGENETs.split_train_attack()\n",
    "# print(f\"INFO: Dataset loaded successfully - {IMAGENETs}\")\n",
    "\n",
    "CIFAR100s = DatasetHolder(\n",
    "    config=CIFAR100Config,\n",
    "    train=CIFAR100(\n",
    "        root=DATA_ROOT, download=True, train=True, transform=CIFAR100Config.augmentation\n",
    "    ),\n",
    "    test=CIFAR100(\n",
    "        root=DATA_ROOT, download=True, train=False, transform=CIFAR100Config.resizer\n",
    "    )\n",
    ")\n",
    "CIFAR100s.split_train_valid()\n",
    "CIFAR100s.split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {CIFAR100s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfaaf862a9a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_DATASET =  CIFAR10s\n",
    "\n",
    "train_dataset = CHOSEN_DATASET.train\n",
    "valid_dataset = CHOSEN_DATASET.valid\n",
    "test_dataset = CHOSEN_DATASET.test\n",
    "\n",
    "print(f\"INFO: Dataset Size - {CHOSEN_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f6925761e5e63",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd53b09a07e7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 512, 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5748cf1442804",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178d4118a670b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.show_sample_grid(**CHOSEN_DATASET.config.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5ea2a2af0c35d",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcb2d583a637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import (\n",
    "    ViTBase, ViTLarge,\n",
    "    SwinTiny, SwinSmall,\n",
    "    ConvNeXtTiny, ConvNeXtSmall,\n",
    "    ResNet50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3500011660e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetModel = SwinSmall\n",
    "\n",
    "# WandB Initialization\n",
    "# try:\n",
    "#     wandb.finish()\n",
    "# except:\n",
    "#     pass\n",
    "# project = wandb.init(project=\"Exp_\"+CHOSEN_DATASET.config.name.upper(), name=TargetModel.model_name)\n",
    "\n",
    "# Initialize Model (automatically loads ImageNet-1K pretrained weights)\n",
    "TargetModel.dataset_name = CHOSEN_DATASET.config.name\n",
    "model = TargetModel(image_size=CHOSEN_DATASET.config.size, num_classes=CHOSEN_DATASET.num_classes)\n",
    "\n",
    "if ADDITIONAL_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=list(range(DEVICE_NUM, DEVICE_NUM+ADDITIONAL_GPU+1)))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f914f6148a87ae7",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ecd5b0e916b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd00e380308053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = CHOSEN_DATASET.config.epoch\n",
    "LEARNING_RATE = 1e-4, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fd351c87211b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "    tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "\n",
    "            # train_progress.update(1)\n",
    "            # if i != train_length-1: wandb.log({'Acc': avg(train_acc)*100, 'Loss': avg(train_loss)})\n",
    "            print(f\"\\rEpoch [{epoch+1:4}/{EPOCHS:4}], Step [{i+1:4}/{train_length:4}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}\", end=\"\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)  # but not use model loss\n",
    "\n",
    "                val_loss.append(criterion(outputs, targets).item())\n",
    "                val_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        # wandb.log({'Train Acc': avg(train_acc)*100, 'Train Loss': avg(train_loss), 'Val Acc': avg(val_acc)*100, 'Val Loss': avg(val_loss)})\n",
    "        print(f\"\\rEpoch [{epoch+1:4}/{EPOCHS:4}], Step [{train_length:4}/{train_length:4}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}, Valid Acc: {avg(val_acc):.6%}, Valid Loss: {avg(val_loss):.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33998508f5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Save\n",
    "if ADDITIONAL_GPU:\n",
    "    model.module.save()\n",
    "else:\n",
    "    model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762d126105c4809",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a4c75ebeb969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = TargetModel(image_size=CHOSEN_DATASET.config.size, num_classes=len(train_dataset.classes))\n",
    "# Note: Model is already loaded with ImageNet-1K pretrained weights\n",
    "# To load your fine-tuned weights, use: model.load(\"path/to/checkpoint.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5249a208f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
